---
title: 'Coursera: Prediction Assignment Plotly'
author: "A. Paul"
date: "28. Dezember 2017"
output:
  html_document:
    df_print: paged
  keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( echo = TRUE )
```

```{r include=FALSE, comment=FALSE}

## install.packages( "devtools" )
library( devtools )
## devtools::install_github("tidyverse/dplyr")
library( tidyverse )

## install.packages( magrittr )
library( magrittr )

## install.packages( lubridate )
library( lubridate )

## install.packages( reshape2 )
library( reshape2 )

## install.packages( plyr )
library( plyr ) 

## devtools::install_github('hadley/ggplot2')
library( ggplot2 )

## install.packages( plotly )
library( plotly )

## install.packages( caret )
library( caret )

## install.packages( kernlab )
library( kernlab )

## install.packages( raster )
library( raster )

## install.packages( e1071 )
library( e1071 )

## install.packages( Metrics )
library( Metrics )

## install.packages( Hmisc )
library( Hmisc )

## install.packages( AppliedPredictiveModeling )
library( AppliedPredictiveModeling )

## install.packages( MASS )
library( MASS )

## install.packages( freqparcoord )
library( freqparcoord )

## install.packages( rattle )
library( rattle )
```

```{r include=FALSE}
## Functions:
## 
normal_data <- function( my_modFit, min=0, max=0, digits=3 ) {
## Normalization  
   if ( min != 0 && max != 0 ) {
     l_min <- min
     l_max <- max 
   } else {
     l_min <- summary(my_modFit)[1]
     l_max <- summary(my_modFit)[6]
   }
   pr_modFit <- 1 - ( ( l_max - my_modFit ) / ( l_max - l_min ) )
## Rückgabe   
   round( pr_modFit, digits = digits )
}
```


## Executive Summary

The report explore die relationship between some variables of data from acelerometers on the belt, forearm, and dumbell of 6 participants. The goal is to predict the "classe" variable. For the prediction of the 20 predefined test data sets the method Support Vector Machine is used and delivers good results.


## Data Preparation und Data Expoloration

#### Data Source

More Information about the accelerometer data is available from the website:
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har 

Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The files should be made available in the current directory.

```{r echo=FALSE}
## read data
## training data
l_file_train <- "pml-training.csv"
training <- read.csv( l_file_train )
cat( paste0( "size of ", l_file_train, " : ", nrow(training) ), "\n" )
## test data
l_file_test <- "pml-testing.csv"
testing <- read.csv( l_file_test )
cat( paste0( "size of ", l_file_test, "  : ", nrow(testing) ), "\n" )
```


#### Data Preparation

First, the columns with missing data are eliminated.

```{r echo=FALSE}
## select columns
k <- 0; l_col_sel <- as.vector(NA)
for ( i in 1:ncol(testing) ) {
  if ( ( sum( is.na( training[,i] ) ) > 0.75 * nrow(training) ) || ( sum( is.na( testing[,i] ) ) > 0.75 * nrow(testing) ) ) { 
  } else {
    k <- k + 1
    l_col_sel[k] <- i
  }
}
```

```{r echo=FALSE}
## select columns without missing values
my_train <- dplyr::select( training, l_col_sel ) 
my_test  <- dplyr::select( testing,  l_col_sel )

## names of selected columns
cat( paste("Selected Variables:", "\n") )
cat( paste("--------------------", "\n") )
names(my_train)
```

The target variable "classe" is decomposed into 5 new numerical variables "classe_A", "classe_B", "classe_C", "classe_D" and "classe_E" for the prediction of the classification.

```{r echo=FALSE}
## Target variables
my_train <- dplyr::mutate( my_train, classe_A = 0,
                                     classe_B = 0,
                                     classe_C = 0,
                                     classe_D = 0,
                                     classe_E = 0 )

my_train[ which( my_train$classe == "A") , ]$classe_A <- 1
my_train[ which( my_train$classe == "B") , ]$classe_B <- 1
my_train[ which( my_train$classe == "C") , ]$classe_C <- 1
my_train[ which( my_train$classe == "D") , ]$classe_D <- 1
my_train[ which( my_train$classe == "E") , ]$classe_E <- 1
```


#### Data Exploration

#### Parallel coordinates of explanatory variables

By way of illustration, the explanatory variables are represented in a diagram of parallel coordinates to search for correlations and groupings.

```{r echo=FALSE, fig.height=8, fig.width=12}
freqparcoord::freqparcoord( x=my_train[ , c( 7:59 )] ,m=30, k=20, faceting="classe" ) + coord_flip()
```

#### Correlations

Here is an overview of the correlations of the explanatory variables and the target variables.

```{r echo=FALSE}
## correlations
cor_all <- abs( round( cor( x=my_train[,-c(1,2,5,6,60:65)], y=my_train[,c(61,62,63,64,65)] ), digits=4 ) )
melted_corr_matrix <- melt( cor_all )
```

```{r echo=FALSE, fig.height=8, fig.width=10}
## heatmap
ggplot2::ggplot( data = melted_corr_matrix, aes(x=Var2, y=Var1, fill=value) ) + 
  ggplot2::geom_tile() + ggplot2::xlab( "Target Variables" ) + ggplot2::ylab( "Explanatory Variables" )
```


#### Data for Training, Valuation and Test

```{r echo=FALSE}
inTrain <- createDataPartition( y=my_train$classe, p=0.75, list=FALSE )
my_training <- my_train[inTrain,]
my_validation <- my_train[-inTrain,]
cat( paste0( "training data   : ", dim(my_training)[1], " observations / ", dim(my_training)[2], " variables", "\n" ) )
cat( paste0( "validation data : ", dim(my_validation)[1], " observations / ", dim(my_validation)[2], " variables", "\n" ) )
cat( paste0( "test data       : ", dim(my_test)[1], " observations / ", dim(my_test)[2], " variables", "\n" ) )
```


## Modeling

The classification of the target variables "classe" or "classe_A", "classe_B", "classe_C", "classe_D" and "classe_E" is made by using the method Support Vector Machine. That each expression of the characteristic "classe" is classified by a separate column. The new columns are created as numeric values.
The larger the value, the more likely the classification to the "classe" and the smaller the less likely.

#### cross validation

The training data is divided into 5 subsets.

```{r echo=FALSE}
set.seed(32323)
## folder counts
c_k <- 5
## create folders
folds <- caret::createFolds( y=my_training$classe, k=c_k, list=FALSE, returnTrain=FALSE )
```

```{r echo=FALSE}
cat( paste0("Size of ", c_k, "-folder subsets:", "\n" ) )
table( as.factor( folds ) )
```

#### Classification of variable "classe_A"

The classification of the target variables "classe" or "classe_A", "classe_B", "classe_C", "classe_D" and "classe_E" is made by using the method Support Vector Machine. The first new columns "classe_A" is created as numeric values. The procedure is based on "one vs all". That The expression "A" is classified against all other occurrences of the target variable "classe". For this purpose, the new target variable "classe_A" is introduced. The larger the value, the more likely the classification to the "classe" and the smaller the less likely.

The function "tune.svm" allows the implicit use of cross-validation. Cross validation is used to determine the best model.

```{r echo=FALSE}
l_exp <- my_train[ , c( 7:59, 61 )]
tune.resA <- e1071::tune.svm( classe_A ~ . , data = l_exp,
                              tunecontrol = tune.control( cross=c_k ) )
```

```{r}
svmfitA <- tune.resA$best.model
bestPerformA <- tune.resA$best.performance
```

```{r echo=FALSE}
cat( paste0( "sampling                 : ", tune.resA$sampling, "\n" ) )
cat( paste0( "performance - error      : ", tune.resA$performances$error, "\n" ) )
cat( paste0( "performance - dispersion : ", tune.resA$performances$dispersion, "\n" ) )
cat( paste0( "Error estimation of 'svm' using 5-fold cross validation: ", tune.resA$best.performance, "\n" ) )
```

```{r}
VmodfitA <- predict( svmfitA, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
```

```{r echo=FALSE}
## paste("Validation: residuals for classe_A:")
sum_svmfit <- summary(svmfitA$residuals)
```

```{r}
## predict test data
TmodFitA <- predict( svmfitA, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## knitr::kable( TmodFitA )
```

```{r echo=FALSE}
tune.resA
```


#### Classification of variable "classe_B"

Analogous to the classification of the variable "classe_A".

```{r echo=FALSE}
l_exp <- my_train[ , c( 7:59, 62 )]
tune.resB <- e1071::tune.svm( classe_B ~ . , data = l_exp, kernel="radial",
                              tunecontrol = tune.control(cross=c_k) )
```

```{r echo=FALSE}
svmfitB <- tune.resB$best.model
bestPerformB <- tune.resB$best.performance
```

```{r echo=FALSE}
cat( paste0( "sampling                 : ", tune.resB$sampling, "\n" ) )
cat( paste0( "performance - error      : ", tune.resB$performances$error, "\n" ) )
cat( paste0( "performance - dispersion : ", tune.resB$performances$dispersion, "\n" ) )
cat( paste0( "Error estimation of 'svm' using 5-fold cross validation: ", tune.resB$best.performance, "\n" ) )
```

```{r echo=FALSE}
VmodfitB <- predict( svmfitB, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
```

```{r echo=FALSE}
## paste("Validation: residuals for classe_B:")
sum_svmfit <- rbind( sum_svmfit, summary(svmfitB$residuals) )
```

```{r echo=FALSE}
## predict test data
TmodFitB <- predict( svmfitB, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## knitr::kable( TmodFitB )
```


#### Classification of variable "classe_C"

Analogous to the classification of the variable "classe_A".

```{r echo=FALSE}
l_exp <- my_train[ , c( 7:59, 63 )]
tune.resC <- e1071::tune.svm( classe_C ~ . , data = l_exp, kernel="radial",
                              tunecontrol = tune.control(cross=c_k) )
```

```{r echo=FALSE}
svmfitC <- tune.resC$best.model
bestPerformC <- tune.resC$best.performance
```

```{r echo=FALSE}
cat( paste0( "sampling                 : ", tune.resC$sampling, "\n" ) )
cat( paste0( "performance - error      : ", tune.resC$performances$error, "\n" ) )
cat( paste0( "performance - dispersion : ", tune.resC$performances$dispersion, "\n" ) )
cat( paste0( "Error estimation of 'svm' using 5-fold cross validation: ", tune.resC$best.performance, "\n" ) )
```


```{r echo=FALSE}
VmodfitC <- predict( svmfitC, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
```

```{r echo=FALSE}
## paste("Validation: residuals for classe_C:")
sum_svmfit <- rbind( sum_svmfit, summary(svmfitC$residuals) )
```

```{r echo=FALSE}
## predict test data
TmodFitC <- predict( svmfitC, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## knitr::kable( TmodFitC )
```


#### Classification of variable "classe_D"

Analogous to the classification of the variable "classe_A".

```{r echo=FALSE}
l_exp <- my_train[ , c( 7:59, 64 ) ]
tune.resD <- e1071::tune.svm( classe_D ~ . , data = l_exp, kernel="radial",
                              tunecontrol = tune.control(cross=c_k) )
```

```{r echo=FALSE}
svmfitD <- tune.resD$best.model
bestPerformD <- tune.resD$best.performance
```

```{r echo=FALSE}
cat( paste0( "sampling                 : ", tune.resD$sampling, "\n" ) )
cat( paste0( "performance - error      : ", tune.resD$performances$error, "\n" ) )
cat( paste0( "performance - dispersion : ", tune.resD$performances$dispersion, "\n" ) )
cat( paste0( "Error estimation of 'svm' using 5-fold cross validation: ", tune.resD$best.performance, "\n" ) )
```

```{r echo=FALSE}
VmodfitD <- predict( svmfitD, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
```

```{r echo=FALSE}
## paste("Validation: residuals for classe_D:")
sum_svmfit <- rbind( sum_svmfit, summary(svmfitD$residuals) )
```

```{r echo=FALSE}
## predict test data
TmodFitD <- predict( svmfitD, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## knitr::kable( TmodFitD )
```


#### Classification of variable "classe_E"

Analogous to the classification of the variable "classe_A".

```{r echo=FALSE}
l_exp <- my_train[ , c( 7:59, 65 ) ]
tune.resE <- e1071::tune.svm( classe_E ~ . , data = l_exp, kernel="radial",
                              tunecontrol = tune.control(cross=c_k) )
```

```{r echo=FALSE}
svmfitE <- tune.resE$best.model
bestPerformE <- tune.resE$best.performance
```

```{r echo=FALSE}
cat( paste0( "sampling                 : ", tune.resE$sampling, "\n" ) )
cat( paste0( "performance - error      : ", tune.resE$performances$error, "\n" ) )
cat( paste0( "performance - dispersion : ", tune.resE$performances$dispersion, "\n" ) )
cat( paste0( "Error estimation of 'svm' using 5-fold cross validation: ", tune.resE$best.performance, "\n" ) )
```


```{r echo=FALSE}
VmodfitE <- predict( svmfitE, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
```

```{r echo=FALSE}
## paste("Validation: residuals for classe_E:")
sum_svmfit <- rbind( sum_svmfit, summary(svmfitE$residuals) )
```

```{r echo=FALSE}
## predict test data
TmodFitE <- predict( svmfitE, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## knitr::kable( TmodFitE )
```


## Interpretation and Visualization

#### Apply ml-algorithm to validation cases

Using the Confusion matrix, the quality of the classification is calculated on the validation data.

```{r}
table( as.factor( my_validation$classe ) )
```

```{r echo=FALSE}
## Normalization
l_min <- min( VmodfitA, VmodfitB, VmodfitC, VmodfitD, VmodfitE ) 
l_max <- max( VmodfitA, VmodfitB, VmodfitC, VmodfitD, VmodfitE )
pr_VmodFitA <- normal_data( my_modFit=VmodfitA, min=l_min, max=l_max )
pr_VmodFitB <- normal_data( my_modFit=VmodfitB, min=l_min, max=l_max )
pr_VmodFitC <- normal_data( my_modFit=VmodfitC, min=l_min, max=l_max )
pr_VmodFitD <- normal_data( my_modFit=VmodfitD, min=l_min, max=l_max )
pr_VmodFitE <- normal_data( my_modFit=VmodfitE, min=l_min, max=l_max )
```

```{r echo=FALSE}
cat( paste0("Validation subset: residuals", "\n" ) )
dimnames(sum_svmfit)[[1]] <- c( "classe_A", "classe_B", "classe_C", "classe_D", "classe_E" )
print( round( sum_svmfit, digits=4 ) )
```

```{r echo=FALSE}
library( dplyr ) 
```


```{r echo=FALSE}
my_Vres1 <- as.data.frame( svmfitA$residuals ) %>% dplyr::mutate( residuals=svmfitA$residuals, classe="classe_A" ) %>% dplyr::select( residuals, classe )
my_Vres2 <- as.data.frame( svmfitB$residuals ) %>% dplyr::mutate( residuals=svmfitB$residuals, classe="classe_B" ) %>% dplyr::select( residuals, classe )
my_Vres3 <- as.data.frame( svmfitC$residuals ) %>% dplyr::mutate( residuals=svmfitC$residuals, classe="classe_C" ) %>% dplyr::select( residuals, classe )
my_Vres4 <- as.data.frame( svmfitD$residuals ) %>% dplyr::mutate( residuals=svmfitD$residuals, classe="classe_D" ) %>% dplyr::select( residuals, classe )
my_Vres5 <- as.data.frame( svmfitE$residuals ) %>% dplyr::mutate( residuals=svmfitE$residuals, classe="classe_E" ) %>% dplyr::select( residuals, classe )
my_Vres <- rbind( my_Vres1, my_Vres2, my_Vres3, my_Vres4, my_Vres5 )
```

```{r echo=FALSE, fig.height=3, fig.width=6}
## validation subset: boxplot of residuals
library(ggplot2)
l_min <- round( min(my_Vres$residuals), digits=1 ) - 0.1
l_max <- round( max(my_Vres$residuals), digits=1 ) + 0.1
ggplot2::ggplot( my_Vres, aes( x=classe, y=residuals ) ) + geom_boxplot() + 
  ggtitle( "Residuals of validation set" ) + ylim( c( l_min, l_max ) ) + coord_flip()
```

```{r echo=FALSE}
cat( paste0( "Error estimation of Support Vector Maschine using 10-fold cross validation:", "\n" ) )
cat( paste0( "---------------------------------------------------------------------------", "\n" ) )
cat( paste0( "classe_A : ", round( bestPerformA, digits = 5 ) ), "\n" )
cat( paste0( "classe_B : ", round( bestPerformB, digits = 5 ) ), "\n" )
cat( paste0( "classe_C : ", round( bestPerformC, digits = 5 ) ), "\n" )
cat( paste0( "classe_D : ", round( bestPerformD, digits = 5 ) ), "\n" )
cat( paste0( "classe_E : ", round( bestPerformE, digits = 5 ) ), "\n" )
```

```{r echo=FALSE}
l_view_VFit <- cbind( pr_VmodFitA, pr_VmodFitB, pr_VmodFitC, pr_VmodFitD, pr_VmodFitE )
l_view_VFit <- round( l_view_VFit, digits=3 )
```

```{r echo=FALSE}
df_V_robust <- data.frame()
k <- nrow( l_view_VFit )
Vclasse <- vector( )
Vprob   <- vector()
for ( i in 1:k ) {
  j <- which.max( l_view_VFit[i,] )
  if ( j == 1 ) { Vclasse[i] <- "A" }
  if ( j == 2 ) { Vclasse[i] <- "B" }
  if ( j == 3 ) { Vclasse[i] <- "C" }
  if ( j == 4 ) { Vclasse[i] <- "D" }
  if ( j == 5 ) { Vclasse[i] <- "E" }
  Vprob[i] <- round( as.numeric(l_view_VFit[i,j]) / sum(as.numeric(l_view_VFit[i,])), digits=2 )
## data.frame df_robust
  l_classe     <- as.character(Vclasse[i]) 
  l_classify   <- as.numeric( as.character(l_view_VFit[i,j]) ) 
  l_robustness <- as.numeric( as.character(Vprob[i]) )
  dl_robust <- cbind( l_classe, l_classify, l_robustness )
  df_V_robust <- rbind( df_V_robust, dl_robust )  
}  
df_V_robust <- mutate( df_V_robust, classe = as.factor(l_classe),
                                    classify = as.numeric( as.character(l_classify) ),
                                    robustness = as.numeric( as.character( l_robustness) ) )
```

```{r}
caret::confusionMatrix( data=my_validation$classe, Vclasse )
```

The key figures Accuracy = 0.9405 and Kappa = 0.9246 as well as Sensitivity and Specificity are very high for the validation data, so that the classification can run on the test data.


#### Apply ML-algorithm to 20 test cases

The classification of the expression of the "classe" takes place, in which the largest value within a row is determined from the 5 values (majority vote). Based on the magnitude, the reader can immediately see how robust or uncertain the classification is. The larger a value stands out, the more robust the classification. The more equally distributed the values within a row, the more uncertain the assignment.

```{r echo=FALSE}
## Normalization
l_min <- min( TmodFitA, TmodFitB, TmodFitC, TmodFitD, TmodFitE )
l_max <- max( TmodFitA, TmodFitB, TmodFitC, TmodFitD, TmodFitE )
pr_TmodFitA <- normal_data( my_modFit=TmodFitA, min=l_min, max=l_max )
pr_TmodFitB <- normal_data( my_modFit=TmodFitB, min=l_min, max=l_max )
pr_TmodFitC <- normal_data( my_modFit=TmodFitC, min=l_min, max=l_max )
pr_TmodFitD <- normal_data( my_modFit=TmodFitD, min=l_min, max=l_max )
pr_TmodFitE <- normal_data( my_modFit=TmodFitE, min=l_min, max=l_max )
```

```{r echo=FALSE}
l_view_Fit <- cbind( pr_TmodFitA, pr_TmodFitB, pr_TmodFitC, pr_TmodFitD, pr_TmodFitE )
l_view_Fit <- round( l_view_Fit, digits=3 )
```

```{r echo=FALSE}
dl_robust <- data.frame()
df_robust <- data.frame()
k <- nrow(l_view_Fit)
classe <- vector()
zprob  <- vector()
zvalue <- vector()
zcolor <- vector(); zcolor[1:140] <- "white"
zsize  <- vector(); zsize[1:140] <- 0
for ( i in 1:k ) {
## classification
  n <- which.max( l_view_Fit[i,] )
  j <- ( n - 1 ) * k + i
  if ( n == 1 ) { 
    classe[i] <- "A"; zvalue[j] <- l_view_Fit[i,n]; zcolor[j] <- "black"; zsize[j] <- 3 
  } else {
    if ( n == 2 ) { 
      classe[i] <- "B"; zvalue[j] <- l_view_Fit[i,n]; zcolor[j] <- "black"; zsize[j] <- 3 
    } else {
      if ( n == 3 ) { 
        classe[i] <- "C"; zvalue[j] <- l_view_Fit[i,n]; zcolor[j] <- "black"; zsize[j] <- 3 
      } else {
        if ( n == 4 ) { 
          classe[i] <- "D"; zvalue[j] <- l_view_Fit[i,n]; zcolor[j] <- "black"; zsize[j] <- 3 
        } else {
          if ( n == 5 ) { 
            classe[i] <- "E"; zvalue[j] <- l_view_Fit[i,n]; zcolor[j] <- "black"; zsize[j] <- 3 
          }
        }  
      }
    }
  }  
  zprob[i]  <- round( as.numeric(l_view_Fit[i,n]) / sum(as.numeric(l_view_Fit[i,])), digits=2 )
## classe  
  o <- ( 6 - 1 ) * k + i
  zvalue[o] <- classe[i]; zcolor[o] <- "black"; zsize[o] <- 4
## robustness  
  m <- ( 7 - 1 ) * k + i
  zvalue[m] <- zprob[i];  zcolor[m] <- "black"; zsize[m] <- 3   
## data.frame df_robust
  l_classe     <- as.character(classe[i]) 
  l_classify   <- as.numeric( as.character(zvalue[j]) ) 
  l_robustness <- as.numeric( as.character(zprob[i]) )
  dl_robust <- cbind( l_classe, l_classify, l_robustness )
  df_robust <- rbind( df_robust, dl_robust )
}
l_view_Fit <- cbind( l_view_Fit, replicate( k, "   " ), classe, zprob )
colnames(l_view_Fit) <- c( "classe A", "classe B", "classe C", "classe D", "classe E", "   ", "classe", "robustness" )
df_robust <- mutate( df_robust, classe = as.factor(l_classe),
                                classify = as.numeric( as.character(l_classify) ),
                                robustness = as.numeric( as.character( l_robustness) ) )

```


##### Presentation as a table

```{r echo=FALSE}
knitr::kable( l_view_Fit, digits=2 )
```

Notice:
The classification of test cases 3) and 11) does not seem to be robust. The values for robustness are relatively low at 0.35 and 0.36. In particular in test case 3) the classification of the expression "C" with 0.39 against "A" with 0.37 and "B" with 0.15 is to be described as weak


##### Presentation as Heatmap

In the graphical representation of the classification, the "weak" values of the test cases 3) and 11) become immediately visible.

```{r echo=FALSE}
## l_view_T_Fit <- cbind( TmodFitA, TmodFitB, TmodFitC, TmodFitD, TmodFitE, 0, zprob )
l_view_T_Fit <- cbind( pr_TmodFitA, pr_TmodFitB, pr_TmodFitC, pr_TmodFitD, pr_TmodFitE, 0, zprob )
colnames(l_view_T_Fit) <- c( "classe_A", "classe_B", "classe_C", "classe_D", "classe_E", "classe", "robustness" ) 
l_view_T_Fit <- round( l_view_T_Fit, digits=2 )
```

```{r echo=FALSE}
library( reshape2 )
my_df <- l_view_T_Fit %>% reshape2::melt() %>% as.data.frame %>% dplyr::mutate( Var1 = as.factor(Var1) )
## NA value for ggplot
zvalue[is.na(zvalue)] <- 0
zcolor[is.na(zcolor)] <- 0
zsize[is.na(zsize)] <- 0
```

```{r echo=FALSE}
ggplot2::ggplot( data = my_df, aes( x=Var2, y=Var1, fill=value ), col = "yellow" ) +
  scale_fill_gradient2( high = "green", mid = "white" ) +
  geom_tile( color = "white") + xlab( "target variables" ) + ylab( "record" ) +
  geom_text( aes( Var2, Var1, label=zvalue ), color=zcolor, size = zsize )
```


##### Visualization of the robustness for the classifiers 

###### Visualisazion for test set

```{r echo=FALSE}
l_today <- Sys.Date()
paste0( "Visualization of robustness ( ", format( l_today, format="%B %d %Y"), " )" )
```

From the classification of the test data stood out two records with a weak robustness. The following graphics show how the robustness of the classification of the expressions of the variable "classe" behaves.

```{r echo=FALSE}
library( ggplot2 )
library( plotly )
```

```{r}
robust_scatter <- df_robust %>%
  ggplot2::ggplot( aes( x = classify, y = robustness, color = classe ) ) + 
  geom_point() +
  ggtitle( "Visualization of robustness" ) +
  xlab( "classify" ) + ylab( "robustness" ) +
  xlim( c(0,1) ) + ylim( c(0,1) )

plotly::ggplotly( robust_scatter )
```


###### Visualisazion for validation set

The amount of test records is too small to recognize a pattern. Therefore, we consider the amount of validation data. Interestingly, there is a visible number of classifications with robustness below 0.5 for all expressions.

The interaction of the representation by means of the package "plotly" makes it possible to look deeper into the set of data points deeper into it.

```{r}
robust_scatter <- df_V_robust %>%
  ggplot( aes( x = classify, y = robustness, color = classe ) ) + 
  geom_point() + 
##  geom_smooth() + 
  ggtitle( "Visualization of robustness" ) +
  xlab( "classify" ) + ylab( "robustness" ) + xlim( c(0,1) ) + ylim( c(0,1) )

ggplotly( robust_scatter )
```

The patterns seem to look similar for all expressions of the variable "classe".

```{r fig.height=4, fig.width=12}
robust_scatter <- df_V_robust %>%
  ggplot2::ggplot( aes( x = classify, y = robustness, color = classe ) ) + 
  geom_point() +
  facet_grid( . ~ classe ) +
  ggtitle( "Visualization of robustness" ) +
  xlab( "classify" ) + ylab( "robustness" ) + xlim( c(0, 1) ) + ylim( c(0, 1) )

plotly::ggplotly( robust_scatter )
```

The boxing plots of robustness per expression show that the classification of "C" drops significantly over the others.

```{r}
robust_boxplot <- df_V_robust %>%
  ggplot2::ggplot( aes( x = 1, y = robustness, color = classe ) ) + 
  geom_boxplot() +
  facet_grid( . ~ classe ) +
  ggtitle( "Visualization of robustness" ) +
  theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank())

plotly::ggplotly( robust_boxplot )
```


